{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T19:13:00.374755Z","iopub.execute_input":"2024-07-12T19:13:00.375538Z","iopub.status.idle":"2024-07-12T19:13:01.475156Z","shell.execute_reply.started":"2024-07-12T19:13:00.375503Z","shell.execute_reply":"2024-07-12T19:13:01.474242Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/tiny-shakespeare/dataset/input.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"#Imports\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim\n#----------------\n\n#Hyperparams\nbatch_size = 64\nblock_size = 256\nsteps = 10000\neval_interval =  500\nlearn_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_steps = 200\nn_embd = 384\nn_heads = 6\nn_layer = 6\ndropout = 0.2\n#-----------------\n\n#Reproducibility\ntorch.manual_seed(1337)\n#-----------------\n\n#Load dataset\nshakespeare_filepath = r'/kaggle/input/tiny-shakespeare/dataset/input.txt'\nwith open(shakespeare_filepath, 'r', encoding='utf-8') as f:\n    text = f.read()\n#----------------\n\n#Viewing dataset\nlen(text)\nprint(text[:1000])\n#---------------\n\n#Tokenization and Model vocabulary\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nstoi = { ch: i for i,ch in enumerate(chars)}\nitos = { i: ch for i,ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n#----------------\n\n#Train and Validation Splits\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n#---------------\n\n#Data Batching\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\n#--------------\n\n#Data loading\ndef get_batch(split):\n    data = train_data if split =='train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size, ))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    \n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_steps)\n        for k in range(eval_steps):\n            X , Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module): #Module head for a single head of self-attention\n    #Single head of self-attention\n    \n    def __init__(self, head_size) : #Initializing our head module, that takes the head_size as a parameter\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False) #Our key vector, which will be used by our tokens to communicate what it is (it's context) K - the index/tags associated with each document\n        self.query = nn.Linear(n_embd, head_size, bias =False) #Our query vector, which will be the information that our token is looking for Q - the question, or search query that our token inputs\n        self.value = nn.Linear(n_embd, head_size, bias =False) #Our value vector, which will be the actual information that our token wants to retrieve V - the actual contents of the documents you want to retrieve based on the question/query.\n        \n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #Register buffer/function tril so we can call it later in the masking, as it is not a parameter of the module\n        \n        self.dropout = nn.Dropout(dropout) #Dropout layer, where some nodes are shut off/dropped out to prevent our model of overfitting to the data\n        \n    def forward (self, x):\n        B, T, C = x.shape #Unpacking Batch, Time , Channels from x shape\n        k = self.key(x) #calling our linear Key function on x to produce (B, T, C) key vector\n        q = self.query(x) #calling our linear query function on x to produce (B, T, C) query vector\n        \n        wei = q @ k.transpose(-2, -1) * C**-0.5 #our Weights, which will be used to aggegrate our value vector and give us information on tokens with high affinities, matches with our current token's query note - we are dividing our query and key dot product but sqrt of head_size, this is to ensure gaussian distribution in our weights---- (B, T, C) @ (B, C, T) ---> (B, T, T)\n        wei = wei.masked_fill(self.tril[ : T, : T ] ==0, float('-inf')) #Masking function, this is a decoder head, so tokens from the future can not communicate with past tokens (would be giving away the correct answer to our model). note - encoder heads can have future tokens communicate with past tokens, as they are used for sentiment analysis. ----- (B, T, T)\n        wei = F.softmax(wei, dim=-1) #Softmax function to normalize our weights to sum to 1 ------ (B, T, T)\n        \n        wei = self.dropout(wei) #Dropout layer to prevent overfitting to data\n        \n        v = self.value(x) #calling our linear value function on x to produce (B, T, C) value vector\n        out = wei @ v #Aggregation of value vector by the weights. --------- (B, T, T) @ (B, T, C) ------> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    #Multiheaded self-attention block\n    \n    def __init__(self, num_heads, head_size): #Initialized with number of heads and head size as parameters\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) #Definition of heads in our Multiheaded Attention block\n        self.proj = nn.Linear(in_features=n_embd, out_features=n_embd) #Linear projection layer, as skip connection to the Add & Norm layer\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1) #Concatenation of our heads in out as our Multihead Self-Attention block\n        out = self.proj(out)\n        out = self.dropout(out)\n        return out\n        \nclass FeedForward(nn.Module):\n    #Feedforward Layer (A simple MLP with ReLU activation between Linear layers) / Basically convolution in our transformer\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential( #a sequential container of our modules, that will add modules as they are passed below.\n            nn.Linear(n_embd, 4*n_embd), #Linear layer, note - Dimensionality is 4x in the feedforward layer, and reduces back from 4x with fan_out in last layer\n            nn.ReLU(), #Activation layer in our FFwd layer\n            nn.Linear(4*n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n        \n    def forward(self, x): \n        return self.net(x)\n    \nclass Block(nn.Module):\n    #Decoder-Only transformer block\n    \n    def __init__(self, n_embd, n_heads): #Initialized to take number of embedding dimensions and number of heads as parameters\n        super().__init__()\n        head_size = n_embd // n_heads #Head size, calculated from n_embd and n_heads\n        self.sa = MultiHeadAttention(n_heads, head_size) #Calling our MultiheadAttention Module as our Self-Attention in the transformer\n        self.ffwd = FeedForward(n_embd) #Calling the Feedforward module, as the Feedforward layer in our transformer\n        self.LN1 = nn.LayerNorm(n_embd) #Layer Normalization, note, very similar to Batch Norm, only difference is that LayerNorm is applied to rows (1th dimension)\n        self.LN2 = nn.LayerNorm(n_embd) #Note we have two Layer Norms, 1 is applied before Self-attention layer and the other is applied before the FFwd layer\n    \n    def forward(self, x):\n        x = x + self.sa(self.LN1(x)) #Our skip/residual connection, this creates a highway for the grad to flow through during backprop. Essentially, the gradient flows unchanged through x during backprop, where we fork off and do Communication on the side\n        x = x + self.ffwd(self.LN2(x)) #Similar to the above comment, A skip/residual connection allowing gradient to flow through during backprop, allowing us to fork off and do Computation on the side.\n        return x #Output of our transformer block\n    \nclass BigramLM(nn.Module): #Our model module, with subclass modules nested within\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #Embedding table using nn.Embedding by PyTorch of size (Vocab_size, Vocab_size)\n        self.postional_embedding_table = nn.Embedding(block_size, n_embd) #Positional encoding\n        self.blocks = nn.Sequential( *[Block(n_embd=n_embd, n_heads=n_heads) for _ in range(n_layer)]) #Sequential ccontainer of our Block module, which will iterate over the number of layers and create the transformer Blocks as required\n            #Block(n_embd=n_embd, n_heads=4),\n            #Block(n_embd=n_embd, n_heads=4),\n            #Block(n_embd=n_embd, n_heads=4),\n            #Block(n_embd=n_embd, n_heads=4),\n            #nn.LayerNorm(n_embd), \n        \n        #Above is our blocks, Block contains our Self-Attention (communication) and Feedforward (computation) mechanisms, and it takes the number of embedding dimensions and number of heads as parameters\n        \n        self.LN_f = nn.LayerNorm(n_embd)\n        #self.sa_heads = MultiHeadAttention(8, n_embd // 8) #Multihead Self-Attention heads for communication\n        #self.ffwd = FeedForward(n_embd) #Feedforward for computation of information on per-token level\n        self.lm_head = nn.Linear(n_embd, vocab_size) #Language model head\n        \n        \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        #idx and targets are both (B, T) tensors of integers\n        \n        tok_emb = self.token_embedding_table(idx) #(B, T, C)\n        pos_emb = self.postional_embedding_table(torch.arange(T, device=device)) #(T, C)\n        x = pos_emb + tok_emb #(B, T, C)\n        x = self.blocks(x)\n        x = self.LN_f(x) #Final LayerNorm  (B, T, C)\n        #x = self.sa_heads(x) #Feeding our token and postional embeddings into our self-attention head. ------ (B, T, C)\n        #x = self.ffwd(x) #Performing the feedforward for computation ----- (B, T, C)\n        logits = self.lm_head(x)#plucking our logits using lm_head function and our resulting embeddings from our self_attention head. ------- (B, T, vocab_size)\n        \n        if targets is None:  #Used for genration purposes, when we are not training and no ground truth targets are fed, then the loss is not calculated\n            loss=None\n        else:\n            B, T, C = logits.shape #Unpacking the logits into Batch, Time and Channel\n            logits = logits.view(B*T, C) #Stretching our logits shape, in this case multliplying B*T, C --> (B,C) tensor as required by our loss function that expects 2 dimensions (Batch, Channel)\n            targets = targets.view(B*T) #Stretching our targets similar to our logits, creating one dimension of B*T --> (B)\n            loss = F.cross_entropy(logits, targets) #Calculatin our loss, logits (B,C) against our targets (B)\n            \n        return logits , loss\n        \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            \n            idx_cond = idx[ : , -block_size : ] #cropping our index to the last block sizen index/position\n            \n            logits, loss = self(idx_cond)\n            logits = logits[ : , -1, : ]\n            probs = F.softmax(logits, dim=1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n            \n        return idx\n\nmodel = BigramLM()\nm = model.to(device)\n\n#output_tokens = m.generate(torch.zeros((1, 1) , dtype = torch.long), max_new_tokens = 100)\n#print(f'inital generation (before training): {decode(output_tokens[0].tolist())}')\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=learn_rate)\n\nfor iter in range(steps):\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        #print(f'step {iter} : train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n    \n    xb, yb = get_batch('train')\n    \n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\nprint(loss.item())\n\n\noutput_tokens = torch.zeros((1,1) , dtype = torch.long, device=device)\nprint(decode(m.generate(output_tokens, max_new_tokens=500)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:13:49.980526Z","iopub.execute_input":"2024-07-12T19:13:49.981027Z","iopub.status.idle":"2024-07-12T20:55:48.399315Z","shell.execute_reply.started":"2024-07-12T19:13:49.980998Z","shell.execute_reply":"2024-07-12T20:55:48.398234Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n0.918272078037262\n\nMakes like deep and bear love out their deaths:\nAnd in royal scocks, this fovenous Frience,\nWho mean to be much to great Saint French after.\n\nBALTHASTINGS:\nAll all resolute slaid of is but of honest,\nDiving annally and a foremand danger\nBetwixt this raint his throne from breast,\nAnd brings them word vengeance from whence finds,\nWe that it interrited there sensent.\n\nKING RICHARD III:\nAnd this denied lie in the hours\nThat thou wilt have the curse of the seal:\nThere's not the gods, but in his purpo\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(output_tokens, max_new_tokens=10000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-12T21:21:08.361197Z","iopub.execute_input":"2024-07-12T21:21:08.361737Z","iopub.status.idle":"2024-07-12T21:24:01.169553Z","shell.execute_reply.started":"2024-07-12T21:21:08.361705Z","shell.execute_reply":"2024-07-12T21:24:01.168619Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n\nPOLIXENES:\nNo more, one some can others every one.\n\nVOLUMNIA:\nTwould they were Lewis by him wears, like Henry,\nAnd leave Aumerle with brest supper their grace.\n\nMENENIUS:\nMadam not, my fellow,\nFather, methoughts, thy companions made\nThat, Jove removed, more worthy than than he,\nWhich greater than Edward can undoub'd him come.\nO, tenty thou to that he is hot,\nShall keep thy hands death to go away\nEither's with ride again? Thou hast made me for thee,\nWhere in dreams when thou hast act of Clarence to his ears\nMade to them by Henry lips and tonguish?\nThey follow'd shungle what say'st thou dost a prince,\nEven in my fear of Richmond?\n\nSherrah, my love, to approvoke me not with a\ngreat: but it was\nThe cannot crowns; go at it not we;\nBut, as the impout better the changeared in my conces\nthe opport; scarces cried by out the pounts, but\nAnd a fortning in great matter seem sorry.\n\nHERMIONE:\nWhy, so!\nTorse my inworthy shall I do cur soft and unto\nThat speaks on my boons with the else gates:\nAppothecarital hurly bent and breath themes,\nSlander victory what is decreetly and ne'er:\nThat I have, by this doublet love the sun:\nRight to my heart\nMay wink attend toward groans them night.\n\nBENVOLIO:\nRomeo, her heart's to stand in this man.\n\nBAMPTISTA:\nIs Romeo there.\n\nBAReather the less, avouch pred feeling builts,\nDivided before himost he way?\n\nMERCUTIO:\nNo, if you good cousin, I pray you should\nBnot that Petruchancast Plashy to hear\nUnder that danger'd in our knave dreams,\nTrawing your courteous against your inject,\nYou went your suffering to egg for force.\n\nEDWARD:\nWhat would seem that, like thou nept thou art,\nFrom on that, if unnatural duke in my forward.\nI was it for mine hath to betch'd him.\n\nMONTAGUE:\nHow would the manor have been before this,\nBy Jupiter, I then depoted by night:\nA know what hath he there won,\nWho payss that he is warlike.\n\nPRINCE:\nThe office which will being enough;\nCommit what he would murder me, 'for him.'\n\nPOLIXENES:\nSeal't, shall stir upon him;\nEven like him seen the glorious dark out\nOf reasonacret: the Capitol, the put him\nWith my poor instantly.\n\nCAMILLO:\nThe uggrant is cencuit of occasion.\n\nAll:\nI will shake our shears in Vienna field,\n\nAll Chiars; it is held by your dishonour\nCannot near all yourself.\n\nCLyman:\nWe cannot have been any friends you with me.\n\nServant:\nAy, it may more want for us; and so that\ngs havingmen.\n\nServant:\nDoubed, sir? why are your companions?\n\nROMEO:\nFor this, sir, I will be found and as old,\nWithout you have been deme--intend now pleaced\nentire to your eyes? Shall we do fret.\nWe have scarce away down and I said\nFor queen'd the why the watery sun; and if\nI have been can spoke, 'twould urite away:\n'Tell him there's like to sg, when I wake,\nAnd whether I will bar--than instrument,\nWith grudge heated with bright ground eye, it banish'd,\nOr, by the tears and finds the languish cplung regreet\nAnd envy the capture of our nights for men,\nBut ere the windows or waking o' the way\nThe shly by men in otherward; it shall most crown\nThan determine the veins well our birth tongue\nMay fight: proud upon him fright fair breath\nAnd once rejoice to thus public. Now, lords?\n\nMOPSA:\nSovery true; then craves; I will I will or the\nthrest house whereon we are turns to what the courts\nours with a rest.\n\nDORCAS:\nWe enjoy, the greatest are the crown, my lord\nmakes lawthers' heavens: there is advice, if\nthey must by great,--\nHunting your grace, givage fell'd.\n\nAUFIDIUS:\nNay, we'll satisfy yonders;\nAnd no more lecs to fight in shame.\n\nCORIOLANUS:\nYou will, sir.\n\nCORIOLANUS:\nTranio, can you do this disture.\n\nFLORIZEL:\nVery unthank your offer'd faces.\n\nMARIANA:\nHow can you we know the counsel?\n\nCAMILLO:\nFit you in your father's morning truth, if\nyou have told me, had you sound to impose your general,\nAnd sent for your divining frantest custom?\n\nLADY CAPULET:\nIt be your petents, with the maid\nThat he may beseech,--like like much mely as by,\nWhich they can do ather diverser concealting,\nAgainst the better.\n\nCAMILLO:\nSir, as well say some certain,\nIf they say, to the cull, till lead money\nShould interchangmen of men, their other poor deeds.\n\nCAMILLO:\nYou have done so long as thus moves yours\nwhen mean root wrong.\n\nANGELO:\nDeath he might thou livest;\nAnd, prepare not the leisure, canst thou yet.\n\nPERDITA:\nI for my good sovereign,\nSo farewell.\n\nJOHN OF GAUNT:\nIt is not more better my canker'd to bear\nA lean nor less nest, than this my grief repair,\nFor a melting more than my strength,\nThat I, in thousand mould beget it,\nGive me some half a death. Be stood to thee,\nExcept my father, house some that doth bleed,\nTo God, be real and fiend to mend for help.\n\nPRINCE EDWARD:\nThen, farewell, proud.\n\nGLOUCESTER:\nCousin, you muse not to Barthol, fond till King Leon;\nHe shall find them that swear the hand.\nThe queen hath left their help of my father-in-chief\nWithin your sole prosperous souls about of them of\n\nSecond Capitol: your worshieve they know my heir\ndepartnes, your gentle father might be so ingrace,\nI have to beard with woe to their council-hearts.\nCome on, ten terms, and grave them libertier:\nwith strengths, power it then Duke of York before so,\nThey shall proclaim free from a mile house\nWithin by stark. If I should love my hands\nThe king of Henry, so hath truth mights and pour\nDeep-dry, be bidening metrell. Good fail\nThat he was Edward in post his was,\nAnd when the Henry and his Edward's hateful brood,\nNot drop the forced chamberland and twenty,\nTo wield his full course, he should have heard the curse,\nAnd this breathed by the seater, will wend\nOne with a throng dead betwixt thine mind.\nThe and usue on what cease's doth he wails his death.\nCommend me to come to play from foul yisle.\nSay, uncle, who bring you more than you were to-ne?\nMy brother he could forth home to bed,\nWhen he is almost he I stand with more grief;\nAnd all that love shows he wither'd his face,\nAnd, being my great power in the high'st.\nO, thus shall war till think it may and ply them.\n\nPRINCE EDWARD:\nSwart with Turks feed from Ravenspurgh?\nWhen give us his princely glimps;\nThat's emself, over man to appear'd daughters.\n\nCLIFFORD:\nIt stands, my lord; 'till heart them gave me sea\ngarden and we do fruit with me: on her plack Hercuita\nAs oft divenry. What thus I see? So many means,\nShe fights must not be as he is up within.\nBut O, who canst her? What what is there?\nMy tongue shall be weary for nece will declines?\n\nFirst Murderer:\nWhat if these were ere this holy man?\n\nSecond Murderer:\nWe should say them, no more lies in their general,\nAnd we beat their to take to her bead doth haste\nHe interceded by his evil: make himself\nHath lost-firm'd his connert to her shall beed;\nSay that he should proceed he call a forthworn\nFavourable imaginary: I must be not so tittend.\n\nDUKE VINCENTIO:\nRight, I promised her thereof, to fight our dumble.\n\nDUKE VINCENTIO:\nThere slander of your grace turnsed upon one you.\n\nQUEEN ELIZABETH:\nYour pardons to the weary weeds that love them;\nBut therefore, troud the nothing hath wrong'd me.\n\nDUKE VINCENTIO:\nI have it boon and like to you: if fall them\nnext being their eyes on thing one between a tide,\nif the people's censure upon their side, he shall doth war\nhim for her life.\n\nANGELO:\nYe are the place: confession he will not\nhome.\n\nISABELLA:\nMost interate hath but devised with me possibly;\nMy poor is the chaste, King Richard,\nAnd all the thirty househouses of live and the chair\nFives makes gross; but she lightly streaks nothing.\nThe nettles of stilen bettle field the selfsame\nShould set me death, to that seems earth words and say.\nThis soothes doth march and the scatte.\nEven thrice that my mother were alms;\nFor stand to their beholding loves our blood,\nTo bloody this leanness'd revemn'd likeness.\n\nFRIAR LAURENCE:\nHold, sail, my lord,\nWith hallower'd already.\n\nFRIAR LAURENCE:\nHold, thou wouldst of goest,\nAt thy secret with my heart and either neat,\nWarwick brideg out, return me to my brow:\n'Tis time to cry when to the lion sleep ingage.\n\nBALTHAR:\nNo holy pelter,--\n\nSTA:\nBesides, my green less, friar, as I should think,\nAnd been prepare upon my faces;\nWhich are they mu may forget you.\n\nFRIAR LAURENCE:\nI bless the offer that which was else and mine\nYou first impression and me twenty thoughts,\nEven thou so remember them weep.\n\nFRIAR LAURENCE:\nThese eyes did stumble: she will doth strike\nNotion himself and so young; but so strives as trong\nAs may make a foren whose under I must,\nEven to suppose gentle and the actor. Or\nto hears, Signior Give her divine him,\nMakes an arvel perfeit or course hot? What\nsay you, Tranio, there have been loved in the number,\nand call'd your fine to whip and vice, nor know but\ngetter, though all this instrument.\n\nServant:\nWe thought it were it like one of your world\nthat my brave: he is a nobles; here it do\npleased it to your knowledge: for this nobour thanks\nstone up himself more and the neither of their wealth\nand livery lord.\n\nPOLIXENES:\nPraise your poor quart, forget to us twain;\non suffering men to pale, some back.\n\nLEONTES:\nShall we wounded?\n\nPOLIXENES:\nTo those that, as he weights that he\nWill joy concestly requiret him for.\n\nLEONTES:\nWe may never lay my so,\nWhere's the prince you mistake is my daughter.\nIf he be just on Elbower my liege,\nHath both got put on him, he's in request\nAnd first thunder with the sharphing eds wandering sit.\n\nFirst Murderer:\nWhat at the soul did between, the veins\nTo the Lord William Brandon, this blood\nFrom Rance twice put his dare from your city;\nWith all stand to him depart; Scrimand,\n'Favour breakful souls and mannerly dury when took,\nThat he is, he did pilgrish with a child,\nTo hide earth he the bogut of my edge,\nBecome the duke, enemy, or had mercy:\nOr in defect, because I had a quiet,\nAnd she is falsehood provoked her brother's horse.\nWhere, hark how the king's knees no borrow's court?\nDid  not the new the English kens,\nAnd who doth if Warwick dare rid weep the siege?\n\nQUEEN ELIZABETH:\nSound shall the stones thereof be bolt\nWith breaks, thou tie teech withing pil\n","output_type":"stream"}]}]}